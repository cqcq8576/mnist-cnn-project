"""
notebooks.04_evaluate_model çš„ Docstring
é˜¶æ®µ4ï¼šæ¨¡å‹è¯„ä¼°ä¸æ€§èƒ½åˆ†æ
"""
import sys
sys.path.append('../src')

import torch
import json
from model import SimpleCNN,get_device
from data_loader import MNISTDataLoader
from train import load_checkpoint
from evaluate import (Evaluator,plot_confusion_matrix,plot_per_class_accuracy,
                      visualize_misclassified_samples,visualize_predictions)

print("=" * 70)
print("é˜¶æ®µ4ï¼šæ¨¡å‹è¯„ä¼°ä¸æ€§èƒ½åˆ†æ")
print("=" * 70)

# 1. åŠ è½½æ•°æ®
print("\nã€æ­¥éª¤1ã€‘åŠ è½½æµ‹è¯•æ•°æ®")
print("-" * 70)
mnist_loader = MNISTDataLoader(data_dir='../data', batch_size=128)
mnist_loader.load_data()
train_loader, val_loader, test_loader = mnist_loader.get_data_loaders()

print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_loader.dataset)}")

# 2. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
print("\nã€æ­¥éª¤2ã€‘åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹")
print("-" * 70)
device = get_device()
model = SimpleCNN(num_classes=10)

model_path = '../models/simple_cnn_best.pth'
model, history = load_checkpoint(model, model_path, device)

# 3. åˆ›å»ºè¯„ä¼°å™¨å¹¶è¯„ä¼°
print("\nã€æ­¥éª¤3ã€‘åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹")
print("-" * 70)
evaluator = Evaluator(model, test_loader, device)
results = evaluator.evaluate()

# 4. ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
print("\nã€æ­¥éª¤4ã€‘ç”Ÿæˆè¯¦ç»†åˆ†ç±»æŠ¥å‘Š")
print("-" * 70)
report = evaluator.get_classification_report(results)
print(report)

# ä¿å­˜æŠ¥å‘Š
report_path = '../results/classification_report.txt'
with open(report_path, 'w', encoding='utf-8') as f:
    f.write("MNISTæ‰‹å†™æ•°å­—è¯†åˆ« - åˆ†ç±»æŠ¥å‘Š\n")
    f.write("=" * 70 + "\n\n")
    f.write(report)
    f.write(f"\n\næ€»ä½“å‡†ç¡®ç‡: {results['accuracy']:.4f}%\n")
    f.write(f"æ­£ç¡®é¢„æµ‹:  {results['correct']}/{results['total']}\n")

print(f"âœ“ åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜åˆ°:  {report_path}")

# 5. ç»˜åˆ¶æ··æ·†çŸ©é˜µ
print("\nã€æ­¥éª¤5ã€‘ç»˜åˆ¶æ··æ·†çŸ©é˜µ")
print("-" * 70)
cm = evaluator.get_confusion_matrix(results)
plot_confusion_matrix(cm, save_path='../results/confusion_matrix.png')

# 6. åˆ†ææ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
print("\nã€æ­¥éª¤6ã€‘åˆ†æå„ç±»åˆ«æ€§èƒ½")
print("-" * 70)
per_class_stats = evaluator.analyze_per_class_accuracy(results)

print("\nå„ç±»åˆ«è¯¦ç»†ç»Ÿè®¡:")
print(f"{'æ•°å­—':<8} {'æ€»æ ·æœ¬':<10} {'æ­£ç¡®æ•°':<10} {'å‡†ç¡®ç‡':<10}")
print("-" * 45)
for digit in range(10):
    stats = per_class_stats[digit]
    print(f"{digit:<8} {stats['total']:<10} {stats['correct']:<10} {stats['accuracy']:.2f}%")

# æ‰¾å‡ºè¡¨ç°æœ€å¥½å’Œæœ€å·®çš„ç±»åˆ«
best_digit = max(per_class_stats.items(), key=lambda x: x[1]['accuracy'])
worst_digit = min(per_class_stats.items(), key=lambda x: x[1]['accuracy'])

print(f"\nè¡¨ç°æœ€å¥½:  æ•°å­— {best_digit[0]} (å‡†ç¡®ç‡: {best_digit[1]['accuracy']:.2f}%)")
print(f"è¡¨ç°æœ€å·®: æ•°å­— {worst_digit[0]} (å‡†ç¡®ç‡: {worst_digit[1]['accuracy']:.2f}%)")

plot_per_class_accuracy(per_class_stats, save_path='../results/per_class_accuracy.png')

# 7. åˆ†æé”™è¯¯é¢„æµ‹æ ·æœ¬
print("\nã€æ­¥éª¤7ã€‘åˆ†æé”™è¯¯é¢„æµ‹æ ·æœ¬")
print("-" * 70)
misclassified = evaluator.find_misclassified_samples(results, num_samples=20)

print("\né”™è¯¯é¢„æµ‹æ ·æœ¬ç¤ºä¾‹:")
for i in range(min(10, len(misclassified['indices']))):
    true_label = misclassified['true_labels'][i]
    pred_label = misclassified['predictions'][i]
    confidence = misclassified['probabilities'][i][pred_label]
    print(f"  æ ·æœ¬ {i+1}:  çœŸå®={true_label}, é¢„æµ‹={pred_label}, ç½®ä¿¡åº¦={confidence*100:.2f}%")

# å¯è§†åŒ–é”™è¯¯æ ·æœ¬
visualize_misclassified_samples(misclassified, results,
                                save_path='../results/misclassified_samples.png')

# 8. å¯è§†åŒ–é¢„æµ‹ç¤ºä¾‹
print("\nã€æ­¥éª¤8ã€‘ç”Ÿæˆé¢„æµ‹ç¤ºä¾‹å¯è§†åŒ–")
print("-" * 70)
visualize_predictions(model, test_loader, device, num_samples=16,
                     save_path='../results/prediction_examples.png')

# 9. ä¿å­˜è¯„ä¼°ç»“æœ
print("\nã€æ­¥éª¤9ã€‘ä¿å­˜è¯„ä¼°ç»“æœ")
print("-" * 70)

evaluation_summary = {
    'test_accuracy': float(results['accuracy']),
    'total_samples': int(results['total']),
    'correct_predictions': int(results['correct']),
    'wrong_predictions': int(results['total'] - results['correct']),
    'per_class_accuracy': {int(k): v for k, v in per_class_stats.items()},
    'best_class': {
        'digit': int(best_digit[0]),
        'accuracy': float(best_digit[1]['accuracy'])
    },
    'worst_class': {
        'digit': int(worst_digit[0]),
        'accuracy': float(worst_digit[1]['accuracy'])
    }
}

summary_path = '../results/evaluation_summary.json'
with open(summary_path, 'w', encoding='utf-8') as f:
    json.dump(evaluation_summary, f, indent=4, ensure_ascii=False)

print(f"âœ“ è¯„ä¼°æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")

# 10. æ€»ç»“
print("\n" + "=" * 70)
print("è¯„ä¼°å®Œæˆæ€»ç»“")
print("=" * 70)

print(f"""
âœ“ æ¨¡å‹è¯„ä¼°å®Œæˆï¼

ğŸ“Š æ€»ä½“æ€§èƒ½: 
  â€¢ æµ‹è¯•é›†å‡†ç¡®ç‡:  {results['accuracy']:.2f}%
  â€¢ æ­£ç¡®é¢„æµ‹: {results['correct']}/{results['total']}
  â€¢ é”™è¯¯é¢„æµ‹:  {results['total'] - results['correct']}

ğŸ¯ ç±»åˆ«æ€§èƒ½:
  â€¢ è¡¨ç°æœ€å¥½: æ•°å­— {best_digit[0]} ({best_digit[1]['accuracy']:.2f}%)
  â€¢ è¡¨ç°æœ€å·®: æ•°å­— {worst_digit[0]} ({worst_digit[1]['accuracy']:.2f}%)
  â€¢ å¹³å‡å‡†ç¡®ç‡: {sum(s['accuracy'] for s in per_class_stats.values()) / 10:.2f}%

ğŸ’¾ ç”Ÿæˆæ–‡ä»¶:
  â€¢ åˆ†ç±»æŠ¥å‘Š: results/classification_report.txt
  â€¢ æ··æ·†çŸ©é˜µ: results/confusion_matrix. png
  â€¢ ç±»åˆ«å‡†ç¡®ç‡: results/per_class_accuracy.png
  â€¢ é”™è¯¯æ ·æœ¬: results/misclassified_samples.png
  â€¢ é¢„æµ‹ç¤ºä¾‹:  results/prediction_examples.png
  â€¢ è¯„ä¼°æ‘˜è¦: results/evaluation_summary.json

ğŸ‰ MNISTæ‰‹å†™æ•°å­—è¯†åˆ«é¡¹ç›®å®Œæˆï¼
""")

print("=" * 70)
print("é˜¶æ®µ4å®Œæˆï¼")
print("=" * 70)